"consolidating 300+ files into 5-8, outofmemory exception i have 369 files that need to be formatted and consolidated into 5-8 files before being submitted to the server.  i can't submit the 369 files because that would overwhelm the metadata tables in our database (they can handle it, but it'd be 369 rows for what was essentially one file, which would make querying and utilizing those tables a nightmare) and i can't handle it as one file because the total of 3.6 gb is too much for ssis to handle on our servers...i wrote the following script to fix the issue:..        static void preppaidclaimsfiles().        {.            const string header = ""some long header text, trimmed for so question"";.            const string footer = ""some long footer text, trimmed for so question"";.            //path is defined as a static member of the containing class.            string[] files = directory.getfiles(path + @""split"");  .            int splitfilecount = 0, finalfilecount = 0;.            list&lt;string&gt; newfilecontents = new list&lt;string&gt;();.            foreach(string file in files).            {.                try.                {.                    var contents = file.readalllines(file).tolist();.                    var fs = file.openread(file);.                    if (splitfilecount == 0).                    {.                        //grab everything except the header.                        contents = contents.getrange(1, contents.count - 1);.                    }.                    else if (splitfilecount == files.length - 1).                    {.                        //grab everything except the footer.                        contents = contents.getrange(0, contents.count - 1);.                    }.                    if (!directory.exists(path + @""splitformatted"")).                    {.                        directory.createdirectory(path + @""splitformatted"");.                    }.                    newfilecontents.addrange(contents);.                    if (splitfilecount % 50 == 0 || splitfilecount &gt;= files.length).                    {.                        console.writeline($""{splitfilecount} {finalfilecount}"");.                        var sb = new stringbuilder(header);.                        foreach (var row in newfilecontents).                        {.                            sb.append(row);.                        }.                        sb.append(footer);.                        newfilecontents = new list&lt;string&gt;();.                        gc.collect();.                        string filename = file.split('\').last();.                        string basefilename = filename.split('.')[0];.                        datetime currenttime = datetime.now;.                        basefilename += ""."" + company_name_sethhmmss(currenttime, finalfilecount) + "".txt"";.                        file.writealltext(path + @""splitformatted"" + basefilename, sb.tostring());.                        finalfilecount += 1;.                    }.                    splitfilecount += 1;.                }.                catch(outofmemoryexception oom).                {.                    console.writeline(file);.                    console.writeline(oom.message);.                    break;.                }.            }.        }...the way this works is it reads the split file, puts its rows into a string builder, every time it gets to a multiple of 50 files, it writes the string builder to a new file and starts over.  the company_name_sethhmmss() method ensures the file has a unique name, so it's not writing to the same file over and over (and i can verify this by seeing the output, it writes two files before exploding.)..it breaks when it gets to the 81st file.  system.outofmemoryexception on var contents = file.readalllines(file).tolist();.  there's nothing special about the 81st file, it's the same exact size as all the others (~10mb.)  the files this function delivers are about ~500mb.  it also has no trouble reading and processing all the files upto and not including the 81st, so i don't think that it's running out of memory reading the file, but running out of memory doing something else and it's at the 81st where memory runs out...the newfilecontents() list should be getting emptied by overwriting it with a new list, right?  that shouldn't be growing with every iteration in this function.  gc.collect() was sort of a last ditch effort...the original file that the 369 splits come from has been a headache for a few days now, causing ultraedit to crash, ssis to crash, blank to crash, etc.  splitting it via 7zip seemed to be the only option that worked, and splitting it to 369 files seemed to be the only option 7zip had that didn't also reformat or somehow compress the file in an undesirable way...is there something that i'm missing?  something in my code that keeps growing in memory?  i know file.readalllines() opens and closes the file, so it should be disposed after called, right?  newfilecontents() gets overwritten every 50th file, as does the string builder.  what else could i be doing?"
