"how to optimize reading and comparing lines between multiple files? what will be the best way to make reading the files more efficient? using this..    with open(fname, 'r') as myf:.        for myline in myf:...or..    for glob.glob('*.txt'):...i performed a cprofile stats and this is what i got for a few sorted lines:..ncalls  tottime  percall  cumtime  percall filename:lineno(function).      4/1    0.000    0.000 1895.428 1895.428 {built-in method builtins.exec}.        1  157.201  157.201 1895.428 1895.428 runmyfiles.py:2(&lt;module&gt;).        5 1365.371  273.074 1554.015  310.803 runmyfiles.py:9(readfile).    52727  184.205    0.003  184.205    0.003 {method 'write' of '_io.textiowrapper' objects}.   194515    0.300    0.000   74.091    0.000 /usr/lib/blank3.6/re.py:214(findall).   194515   72.479    0.000   72.479    0.000 {method 'findall' of '_sre.sre_pattern' objects}...i know i should optimize the file reading process but not sure how to go about it. i may need to open all the files at once and compare each line in each file with similar lines in the other files then write the output for the line containing the max value of bb. is this logical considering the stats generated?"
