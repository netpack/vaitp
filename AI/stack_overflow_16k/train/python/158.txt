"web crawler text cloud i need help with a text cloud program i'm working on. i realize it's homework, but i've gotten pretty far on my own, only to now be stumped for hours. i'm stuck on the web crawler part. the program is supposed to open a page, gather all the words from that page, and sort them by frequency. then it's supposed to open any links on that page and get the words on that page, etc. the depth is controlled by a global variable depth. in the end, it's supposed to put all the words from all pages together to form a text cloud. ..i'm trying to use recursion to call a function to keep opening links until the depth is reached. the import statement at the top is only to use a function called gethtml(url), which returns a tuple of the list of words on the page, and any links on the page...here is my code so far. every function works as it should, except for getrecursiveurls(url, depth) and makewords(i). i'm also not 100% sure about the counter(list) function at the bottom...from hmc_urllib import gethtml..maxwords = 50.depth = 2..all_links = []..def geturl():.    """"""asks the user for a url""""""..    url = input('please enter a url: ')..    #all_links.append(url)..    return makelistofwords(url), getrecursiveurls(url, depth)...def getrecursiveurls(url, depth):.    """"""opens up all links and adds them to global all_links list,.    if they're not in all_links already""""""..    s = gethtml(url).    links = s[1].    if depth &gt; 0:.        for i in links:.            getrecursiveurls(i, depth - 1).            if i not in all_links:.                all_links.append(i).                #print('this is all_links in the if', all_links).                makewords(i)#getrecursiveurls(i, depth - 1).            #elif i in all_links:..             #   print('this is all_links in the elif', all_links).              #  makewords(i) #getrecursiveurls(i, depth - 1).    #print('all_links at the end', all_links).    return all_links......def makewords(i):.    """"""take all_links and create a dictionary for each page..    then, create a final dictionary of all the words on all pages.""""""..    for i in all_links:.        finaldict = makelistofwords(i).        #print(all_links).        #makelistofwords(i)).    return finaldict...def makelistofwords(url):.    """"""gets the text from a webpage and puts the words into a list""""""..    text = gethtml(str(url)).    l = text[0].split().    return cleaner(l)...def cleaner(l):..    """"""cleans the text of punctuation and removes words if they are in the stop list.""""""..    stoplist = ['', 'a', 'i', 'the', 'and', 'an', 'in', 'with', 'for',.                'it', 'am', 'at', 'on', 'of', 'to', 'is', 'so', 'too',.                'my', 'but', 'are', 'very', 'here', 'even', 'from',.                'them', 'then', 'than', 'this', 'that', 'though']..    x = [depunc(c) for c in l]..    for c in x:.        if c in stoplist:.            x.remove(c)..    a = [stemmer(c) for c in x]..    return counter(a)...def depunc( rawword ):.    """""" de-punctuationifies the input string """"""..    l = [ c for c in rawword if 'a' &lt;= c &lt;= 'z' or 'a' &lt;= c &lt;= 'z' ].    word = ''.join(l).    return word...def stemmer(word):..    """"""stems the words""""""..    # list of endings.    endings = ['ed', 'es', 's', 'ly', 'ing', 'er', 'ers']..    # this first case handles 3 letter suffixes with a doubled consonant. i.e. spammers -&gt; spam.    if word[len(word)-3:len(word)] in endings and word[-4] == word[-5]:.        return word[0:len(word)-4]..    # this case handles 3 letter suffixes without a doubled consonant. i.e. players -&gt; play.    elif word[len(word)-3:len(word)] in endings and word[-4] != word[-5]:.        return word[0:len(word)-3]..    # this case handles 2 letter suffixes with a doubled consonant. i.e. spammed -&gt; spam.    elif word[len(word)-2:len(word)] in endings and word[-3] == word[-4]:.        return word[0:len(word)-3]..    # this case handles 2 letter suffixes without a doubled consonant. i.e. played -&gt; played.    elif word[len(word)-2:len(word)] in endings and word[-3] != word[-4]:.        return word[0:len(word)-3]..    # if word not inflected, return as-is..    else:.        return word..def counter(list):.    """"""creates dictionary of words and their frequencies, 'sorts' them,.    and prints them from most least frequent""""""..    freq = {}.    result = {}. # assign frequency to each word.    for item in list:.        freq[item] = freq.get(item,0) + 1..    # 'sort' the dictionary by frequency.    for i in sorted(freq, key=freq.get, reverse=true):.        if len(result) &lt; maxwords:.            print(i, '(', freq[i], ')', sep='').            result[i] = freq[i].    return result"
