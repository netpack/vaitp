".replace() randomly not replacing string correctly i am using .rsplit() to split up all the digits in a string after the last comma using further commas. the transformations should be like this:..before:..,000...after:..,0,0,0...i am using the following method to do this:..upl = line.rsplit("","",1)[1:].upl2 = ""{}"".format("","".join(list(upl[0])))...as a comparison, to ensure that the correct substring is being selected to begin with, i am also using this statement:..upl1 = ""{}"".format("""".join(list(upl[0])))...i then print both to ensure that they are both as expected. in this example i get:..up1 = ,000.up2 = ,0,0,0,...i then use a .replace() statement to substitute out my before substring with my after one:..new_var = ''.            for line in new_var.split(""n""):.                upl = line.rsplit("","",1)[1:].                upl1 = ""{}"".format("""".join(list(upl[0]))).                upl2 = ""{}"".format("","".join(list(upl[0]))).                upl2 = str(upl2).                upl1 = str(upl1).                new_var += line.replace(upl1, upl2) + 'n'...in almost all instances of parsed data the old substring is overwritten with the new correctly. however on a few the subbed in string will display as:..,0,00 when it should be ,0,0,0,...can anyone see anything obvious as to why this might be as i am at a bit of a loss...thanks..edit:..here is the scrapy code i am using to generate the data i am manipulating. the issues come from line:..new_match3g += line.replace(spl1, spl2).replace(tpl1, tpl2).replace(upl1, upl2) + 'n'...the full code is:..from scrapy.contrib.spiders import crawlspider, rule.from scrapy.contrib.linkextractors.sgml import sgmllinkextractor.from scrapy.selector import selector.from scrapy.item import item.from scrapy.spider import basespider.from scrapy import log.from scrapy.cmdline import execute.from scrapy.utils.markup import remove_tags.import time.import re.import json...class examplespider(crawlspider):.    name = ""mrcrawl2"".    allowed_domains = [""whoscored.com""].    start_urls = [""http://www.whoscored.com""].    download_delay = 5..    rules = [rule(sgmllinkextractor(allow=('/seasons'),deny=('/news', '/fixtures', '/graphics', '/articles', '/live', '/matches', '/explanations', '/glossary', '/players', 'contactus', 'termsofuse', 'jobs', 'aboutus', 'rss'),), follow=false, callback='parse_item')]..    def parse_item(self, response):..        sel = selector(response)..        regex = re.compile('datastore.prime('history', { stageid: d+ },[[.*?]]?)?;', re.s)..        match2g = re.search(regex, response.body)..        if match2g is not none:.            match3g = match2g.group()..            match3g = str(match3g).            match3g = match3g.replace(""'"", '').replace(""'"", '').replace('[', '').replace(']', '').replace('] );', '') ..            match3g = re.sub(""datastore.prime(history, { stageid: d+ },"", '', match3g).            match3g = match3g.replace(');', '').            #print'-' * 170, 'n', match3g.decode('utf-8'), '-' * 170, 'n'..            new_match3g = ''.            for line in match3g.split(""n""):.                upl = line.rsplit("","",1)[1:].                if upl:.                    upl1 = ""{}"".format("""".join(list(upl[0]))).                    upl2 = ""{}"".format("","".join(list(upl[0]))).                    upl2 = str(upl2).                    upl1 = str(upl1).                    new_match3g += line.replace(upl1, upl2) + 'n'..                    print ""upl1 = "", upl1.                    print ""upl2 = "", upl2..            print'-' * 170, 'n', new_match3g.decode('utf-8'), '-' * 170, 'n'.            print'-' * 170, 'n', match3g.decode('utf-8'), '-' * 170, 'n'....execute(['scrapy','crawl','mrcrawl2'])"
