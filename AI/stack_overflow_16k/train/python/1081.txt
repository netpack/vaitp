"change groupby with join spark sql query to spark dataframe i have written my scripts initially using spark sql but now for performance and other reasons trying to convert the sql queries to pyspark dataframes...i have orders table (orderid,customerid,employeeid,orderdate,shipperid).and shippers  table (shipperid, shippername)..my spark sql query lists the number of orders sent by each shipper:.. sqlcontext.sql(""select shippers.shippername, count(orders.shipperid) as numberoforders . from orders left join shippers on orders.shipperid = shippers.shipperid . group by shippername"")...now when i try to replace the above sql query with spark dataframe,i write this..shippers.join(orders,[""shipperid""],'left').select(shippers.shippername).groupby(shippers.shippername).agg(count(orders.shipperid).alias(""numberoforders"")) ...but i get an error here mostly because i feel aggregate count function while finding count of orderid from orders table is wrong...below is the error that i get:-..""an error occurred while calling {0}{1}{2}.n"".format(target_id, ""."", name), value)""...can someone please help me to refactor the above sql query to spark dataframe ?"
