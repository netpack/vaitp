"docvecs.most_similar() not working properly. won't find docs so, i'm new to blank, and this is one of the first problems i'm solving. i realise i'm in over my head a bit, but i really need this done (and i want to learn!)...i'm trying to classify different aricles based on their product description. for this i'm using gensim doc2vec, along with the usual pandas, numpy etc. ..i have a document of about 6000 unique product descriptions, with their respective article number. i have no difficulty reading the file, formatting and tokenizing it and removing stopwords etc. what is giving me a headache is the acutal model training. to set up the code i'm using a part of the document with only 200 articles, so that the training doesn't take forever each time i get it wrong...i get results when using the model.most_similar(""word"") function (although they are not that great, which isn't strange for such a small sample). what bothers me is that i cannot get the model.docvecs.most_similar([doc_id]) function to work. ..##...reading and formatting file. variable 'data' is the df containing tokenized lists etc........def create_taggeddoc (): #code for creating taggeddocument-list.    tagged = [].    i = 0.    for row in data['no_stops_nbrs']:.        tag = data['artcle'].iloc[i].        tagged.append(taggeddocument(row, tags = tag)).        i += 1..    return tagged...def train_data(): #returns trained model.    #...training model....    return model...model.docvecs.most_similar(artnbr) #the one that doesn't work...the taggeddocument method seems correct (albeit unelegant), as: ..print(tagged[:5])...shows the correct wordlists and tags. ..for some reason though, the last line doesn't work. when i enter the artnbr parameter as a string: eg ..model.docvecs.most_similar('1234')...i get: ..typeerror: '&lt;' not supported between instances of 'str' and 'int'..but when i enter it as an int, eg ..model.docvecs.most_similar(1234)...i get: ..keyerror: ""doc '1234' not in trained set""..what am i not understanding here? ..sorry if the information is lacking, i'll provide more if needed. thanks!"
