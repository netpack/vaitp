"gpt2 train on google colab (ram is not enough) it is the first time that i approach this type of machine learning (finetune) and i have doubts:..1) i have a dataset of about 500m but on google colab i can't load it entirely in ram and mallock error, now the question is: can i split the dataset into smaller parts? will i have less gain on the model? (calculating that the dataset is composed of songs delimited by prefix and truncate tags)..2) if the answer to question 1 is yes: what do you recommend?"
