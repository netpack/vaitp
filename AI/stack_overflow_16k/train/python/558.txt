"queueing work, without using a lot of memory i'm trying to set something up where one thread is writing a list of work and another thread is reading the list and working from it.  this list can be very large so to stop this list being held in memory i want to have it written in a file (or anyway of preserving memory- generators?)...i put together a little runnable example with a sleep in the writer so that the reader can catch up. i'm wondering how i can get the reader to not stop when it ""overtakes"" the writer. i looked at using .seek and .tell but i got weird behaviour and i'm not sure that's the right route...as another question, is this at all a sensible idea? maybe there's a much more elegant way i can queue up a list of strings without using loads of memory...import threading,time..class writer(threading.thread):..  lock= threading.lock()..  def __init__(self,file_path,size):.    threading.thread.__init__(self).    self.file_path= file_path.    self.size= size.    self.i=0..  def how_many(self):.    with self.lock:.      print ""reader starting, writer is on"",self.i..  def run(self):.    f=open(self.file_path,""w"").    for i in xrange(self.size):.      with self.lock:.        self.i=i.      if i%1000==0:.        time.sleep(0.1).      f.write(""%sn""%i).    f.close()..class reader(threading.thread):..  def __init__(self,file_path):.    threading.thread.__init__(self).    self.file_path= file_path..  def run(self):.    f=open(self.file_path,""r"").    line=0.    for line in f:.      pass.    print ""reader got to: %s""%line.strip()...if __name__ == ""__main__"":.  a= writer(""testfile"",2000000).  b= reader(""testfile"").  a.start().  time.sleep(1).  a.how_many().  b.start()"
